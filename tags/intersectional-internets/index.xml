<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>intersectional internets on ALEXANDRIA ((MAIE 2022))</title><link>https://lexahl.github.io/maie/tags/intersectional-internets/</link><description>Recent content in intersectional internets on ALEXANDRIA ((MAIE 2022))</description><generator>Hugo -- gohugo.io</generator><language>en-UK</language><lastBuildDate>Tue, 12 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://lexahl.github.io/maie/tags/intersectional-internets/index.xml" rel="self" type="application/rss+xml"/><item><title>>> Dreaming Disability Justice in HCI Workshop</title><link>https://lexahl.github.io/maie/dreaming-disability-justice-in-hci-workshop/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><guid>https://lexahl.github.io/maie/dreaming-disability-justice-in-hci-workshop/</guid><description>&lt;p>&lt;a href="https://lexahl.github.io/maie/txt/dind.txt" target="_blank">click here for a plain text file of this page →&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://disabilityjusticeinhci.org/" target="_blank">click here to visit the Dreaming Disability Justice in HCI Workshop Website →&lt;/a>&lt;/p>
&lt;p>&lt;i>(Workshop description) In this one-day workshop, we will bring together people doing research to address ableism that takes a justice-oriented and intersectional perspective. We will use this workshop to enrich critical HCI scholarship and challenge western, white, and ableist hegemonies within the field and beyond. This workshop will also be used as a means to go beyond accessibility, acknowledging how disability is present in all facets of HCI, and calling in a diverse range of scholarships to critically reflect on disability justice within their work. Together, we will explore several critical questions, including: How does disability intersect with other overlapping systems of oppression (racism, anti-Blackness, sexism, classism, colonialism, etc) and how does it inform the technologies we design? How does disability justice expand and complicate human-centered approaches in HCI research? What might disability justice look like in HCI? How can we as HCI researchers create and maintain a practice of care in creating access?
&lt;/i>&lt;/p>
&lt;p>🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭 🥭&lt;/p>
&lt;h3> My accepted submission to the workshop: &lt;/h3>
&lt;p>Behind every paywall that my fingers can gracefully jump, there is always an accessibility challenge waiting for me. I have the privilege to get to your academic articles, and they sometimes read as deliciously as a cold mango on a hot day. Not everyone is granted access to your articles, but I often am or fight for it; otherwise, I wouldn&amp;rsquo;t be here right now. I can get to the articles, but then I get stuck in the mud.&lt;/p>
&lt;p>I have been daydreaming of a button. This button would allow me to listen to academic articles more comfortably. I climbed through the web and over the wall; now, let me eat my mango in peace as the sweet juice drips off my lips. My text reader reads your complete email addresses out. My text reader reads the ACM reference formats, the Copywrites, the tables, and all of the page numbers out. My text reader also skips the information that hides between brackets in your articles. How much have I missed? Why do I need to engage with your work this way? Please send me a plain text file of your work. Or send me a voice memo on WhatsApp. It can be more than 5 minutes, I will listen to it all. Call me and tell me about it, and let me hear the sounds from your street.&lt;/p>
&lt;p>Let&amp;rsquo;s think about how we perpetuate colonial aesthetics of visual knowledge production and validation. Let&amp;rsquo;s think about why we need to create unreachable articles in these formats that preach accessibility. Let us come together to create a button that takes in these academic articles and converts the document into a plain text file so I can listen to it. I am daydreaming of this button. What button are you daydreaming of?&lt;/p></description></item><item><title>>> Ideas, Problems, and Resistance Methods Surrounding Data Biases</title><link>https://lexahl.github.io/maie/ideas-problems-and-resistance-methods-surrounding-data-biases/</link><pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate><guid>https://lexahl.github.io/maie/ideas-problems-and-resistance-methods-surrounding-data-biases/</guid><description>&lt;p>&lt;a href="https://lexahl.github.io/maie/txt/datawriting.txt" target="_blank">click here for a plain text file of this page →&lt;/a>&lt;/p>
&lt;h3 id="intentions-experiences-and-encounters-with-data">Intentions, Experiences, and Encounters with Data&lt;/h3>
&lt;p>This piece of writing results from four-ish years of working, academically researching, teaching, organizing, and sharing my knowledge about data and computational technologies. It is essential to situate myself and this work’s emergence within a larger context as all my experiences and views came from somewhere or someone. This is for my family and friends and for whoever comes across this. Data is not just for computer scientists to think about, your data is something you should have control over, and it is important to talk about. I hope this helps us have more conversations.&lt;/p>
&lt;p>My interest in data started in 2010 when I worked part-time in a data entry position. The repetition and monotony of data entry created a peaceful place for me. Mathematics (one of my undergraduate majors) is about order and patterns. I also found peace here, as I had understood numbers and data as being so disconnected from my own life.&lt;/p>
&lt;p>In my first experience working as a data analyst, I was tasked with analyzing environmental data and creating a more consistent way of collecting data. Carla De Waele, an Environmental Specialist and my first supervisor as a data analyst, made sure that I knew the data I was working with and the people who created it. I was in the Environmental Protection Program at Canadian Nuclear Laboratories, and I did not understand why I needed to meet with people to work with environmental data – the data should be neutral, there should be no bias, it’s not about people, right? I was working with data of all kinds, and I was sent to meet the researchers who had collected it before I could access it. Looking at the measurement tools and where they were placed around the plant showed me that data is not just numbers and could never be neutral. There was somebody who decided it was important to measure the temperature at the plant. Somebody also decided to measure it with this one specific device made in the 60s with an instruction manual that was only in German, and somebody decided where to place this device.&lt;/p>
&lt;p>Data does not exist without us. We create it, give it meaning, and use it for a reason. Data and the uses of data about people are never purely technical or neutral. They are very much intertwined into the cultural, socioeconomic, and political contexts in which they were produced. Data collection, too, is inherently political. The act itself, the appropriation of life, objects, and more in the form of numbers, can be seen as part of an ideology.&lt;/p>
&lt;p>My background and experiences with data go much further in the past, but so does yours. Data is all around us. If you have a birth certificate, you have experience with data. From the moment you were born, you were already categorized into a binary category of “male or female”; if not, maybe you were categorized into a binary “woman or man. For some, this may be fine, but for the 1.7% of the population (Its Intersex Awareness, 2018) who are Intersex, being categorized and constrained into this binary can be harmful and have lifelong effects. Biological sex can be seen as “not a coherent category” (Albert and Delano, 2021), and a decision was made to see it as one. This is just one example of an encounter with data and a problem with modern data practices. The rest of this writing will discuss where data is and isn’t and steps we can take towards better futures alongside data.&lt;/p>
&lt;h3 id="an-example-datagénero">An Example: Datagénero&lt;/h3>
&lt;p>DataGénero is an organisation that seeks to build a sustainable and inclusive data future from and for Argentina. They focus on the data practices that directly affect the lives of women and LGBTQ+ people in Latin America and advise individuals, governments and organisations that work with data. Their work focuses on the specific and cultural contexts in Argentina and Latin America, and they work with una perspectiva de género (a gender perspective, DataGénero).&lt;/p>
&lt;p>A gender perspective in the context of data work means considering gender and other identity intersections, including race, class, sexuality, and ability, at all stages when working with data. This is important when collecting data about people because you almost always encounter gendered experiences. Social and gender norms can be represented, resisted, and reproduced in all social relations. Because of this, it is crucial to consider the dynamics of gender and its intersection with other inequalities when working with people and data about people. The work that DataGénero is doing in Argentina and Latin America is important because it makes certain identities, experiences, and labours visible in the form of data. It is also necessary for policymaking, sharing resources, and providing access, specifically in Argentina, where lots of data grouped by gender does not exist yet. Because numbers are seen by the people in charge as more valid than individual stories, collecting data can be instrumental when advocating for policy and policy changes. It can also help to know the data to more appropriately direct funding and resources.&lt;/p>
&lt;p>When working with data, considering different intersections of identities, as DataGénero does, when working with data is crucial because many of us do not live “single-issue lives” (Lorde, 1984). When intersectional data is missing, it can have real-world meanings for funding, policy-, and access-related decisions, but it does not mean that people are missing. I bring this up because there is some discussion around data and “invisible people”. The term “invisible” refers to the lack of data about a certain group of people. For example, in a call to collect more gender data, Plan International had put out a report entitled “Counting the Invisible.” Similarly, a book about the lack of data about women was titled “Invisible women” (Criado Perez, 2019). This lack of data is understood to render people invisible. I do not agree with this term, nor do I completely agree with the statement by King et al. that “where data are missing, people are missing” (2020). Instead, I believe that it is not the people that are missing but rather a representation of their experiences. The difference here is that even without data, a person can be seen, heard, and respected. Again, these are all decisions that people are making regarding data, who is represented and who isn’t. These are all biases.&lt;/p>
&lt;p>When talking about data, the lack of data grouped by gender is referred to as having a lack of “gender-disaggregated data”. To aggregate data means to group data based on a specific characteristic, often for the purpose of a summary. To disaggregate data is to separate the data based on a certain characteristic. Closer to home in Canada, there was no effective pandemic response for racialised communities and people because of the lack of race disaggregated during the year 2020 of the COVID-19 pandemic. Race is just one identity intersection that can be considered when taking an intersectional approach. Other identity intersections are also often not collected in data, and with this layering of missing identities, even more experiences are not being represented. The constitutional principle of Laïcite in France “guarantees the neutrality of the State” (Gilbert &amp;amp; Keane, 2016). Because of this principle, it fails to collect disaggregated data by ethnicity or religion. Again, making the decision not to include certain identities is due to a bias.&lt;/p>
&lt;p>The work that DataGénero is doing is a positive example of how to work with data about people. It is local, contextual, and the organisers have experience in Argentina. The work also raises a crucial of data bias. Decisions about what data to collect, how to collect it, how to share it, and how to interpret it all have an inherent bias. The interpretation of data can be seen as a very powerful tool. Creating and collecting data can be used to create positive change, as what DataGénero are doing, for liberatory movements, or just for fun. However, data collection and interpretation can also be used in harmful or straight-up creepy ways. When considering and critically engaging with data around you, you might not even be aware of how much of your life is being collected as data.&lt;/p>
&lt;h3 id="data-commodification-and-data-consent">Data Commodification and Data Consent&lt;/h3>
&lt;p>When collecting data about people, many decisions need to be made, and we know this creates a bias. Data about people can also be harmful due to it often being thought of, and treated as, a commodity. The saying &amp;ldquo;if the product is free, you are the product&amp;rdquo; rings true with how social media and advertising today are structured, and this statement is very much linked to data collection.&lt;/p>
&lt;p>In 2017, it was reported that 98.5 percent of Facebook&amp;rsquo;s revenue came from advertisements (Dillet, 2018). The way this is linked to data, is that Facebook collects incredible amounts of data from you, puts you into categories based on this data from your online behaviours, and then sells this information to advertisers. It is sold in a way that doesn&amp;rsquo;t look like you are buying the data about people; it is sold as a way for people to choose who they would like to advertise to. Furthermore, because people can choose who they would like to advertise to, they also have been able to choose who they would not like to advertise to. This can hide jobs, housing, and credit opportunities from marginalized groups, which is a form of discrimination. Having nothing to hide does not prevent you from being affected by what happens with so much data collection. And just because it doesn&amp;rsquo;t affect you does not mean you should not care about it.&lt;/p>
&lt;p>Being grouped into categories based on data about you is very prevalent. For example, Amazon, an e-commerce company, uses &amp;ldquo;real-time personalization and recommendation&amp;rdquo; by collecting a lot of data about you, from how many seconds you spend looking at an item, to where you are located, and much more. These decisions to collect this data about only certain things about you were made with the company&amp;rsquo;s goals in mind. Even though you might get a more personalized experience, you are just being tricked into buying more. With these choices in what to collect from you, Amazon can recommend products for you, sometimes even before you know you want them. This isn&amp;rsquo;t quite machine learning or artificial intelligence yet. It groups people into categories based on the data that is collected.&lt;/p>
&lt;p>Similar experiences have happened at Target, a department store in the United States, over ten years ago. Based on the consumer data that Target had collected, data analysts predicted a pattern that pregnant people often switch to unscented lotion. Because of this, Target would send &amp;ldquo;baby coupons for baby items&amp;rdquo; to customers based on the prediction that they might be pregnant because of their recent shopping changes. This led to customers feeling &amp;ldquo;creeped out&amp;rdquo; and learning about pregnancies they were not previously aware of (Hill, 2012). It would be a bit creepy for a company to know something so personal about a consumer without them sharing it, right? Unfortunately, the data that is about us today goes much further than our shopping habits, and a lot of it is collected without us being aware. Data about us is seen as a commodity; some have even called data &amp;ldquo;the new oil&amp;rdquo; (Joris Toonders, 2014). Yikes – but also in the same way that oil is taken out of the land and extracted in harmful ways, data is also taken without consent.&lt;/p>
&lt;p>Consent is an essential topic in any interaction, and it is also crucial when thinking about data in our lives. Depending on our location, our jobs, our health, the companies that we engage with, and more, our data is constantly being collected by many different groups. Data can be collected and used with our consent, or without our consent. Facebook, for example, collects a lot more data than just what we decide to share on the platform, so they can sell more to advertisers. An example of data collection with consent is sharing your email address to receive email updates from an organization or company you are interested in. In this scenario, you know which data you are sharing and with whom, and we imagine here that it is not taken from you without you knowing. However, this consent is limited in the way that you were probably not be told if your email is shared with others if interest in this company can prevent you from accessing another company, how and when you can remove the consent and get your data back. These are important questions to ask when sharing your data, or when collecting data.&lt;/p>
&lt;p>In the EU, there is the General Data Protection Regulation (GDPR and the UK-GDPR, 2018) which governs the processing of individual data. This is meant to work towards more equitable data practices and make sure one can remove consent and data they shared with others. With this regulation, one often sees a pop-up when visiting a website asking for consent to collect different types of data from the user. In this case, the user should have the ability to refuse their data from being collected or to accept that their data is collected. However, in many of these pop-ups, the button or way to refuse to share data is much more difficult to find. As a result, users can feel frustrated when finding information, they need from the website and reluctantly accept sharing data rather than enthusiastically and voluntarily. For example, when searching on Google, it takes four more clicks to refuse data collection than it does to accept sharing your data. While it is a step in the right direction to ask before taking data, it can feel annoying to click through the complicated policies to decline data collection, influencing the user to share their data unenthusiastically. Consent should be enthusiastic and positive, and as a friend of mine, Cheska, said, &amp;ldquo;consent should be easy&amp;rdquo; (Lotherington, 2022).&lt;/p>
&lt;p>So much of what we consume and what we do is represented in data now. It is hard to avoid it, but it is better to be aware of what is going on.&lt;/p>
&lt;h3 id="critical-approaches-to-data">Critical Approaches to Data&lt;/h3>
&lt;p>To summarise, I believe that contextualising the data, your relation to the data, and recognising the various powers and biases involved with the data can be the best way to resist extractive data collection and be aware of the potential harms. In the same way I needed to be able to read the Table of Nuclides (I would call this the hulk version of the periodic table) before working with the data at Canadian Nuclear Laboratories, thinking of the bigger picture and your relations to the data and who is collecting the data are how to think of data critically. Thinking about Data Justice is another one of the ways.&lt;/p>
&lt;p>&amp;ldquo;Data Justice&amp;rdquo; goes further than just collecting data from more people. Data Justice demands that the lives and experiences of those being represented in the data be prioritised. This means considering a gender perspective and different intersecting identities. Data Justice also ensures that everyone involved in the project with data is held accountable and that power relations are considered when working with data. This means considering data biases, consent, and the commodification of data, among other concerns as mentioned previously. Finally, data justice works to make sure collecting data is not so extractive, unlike taking oil from the ground.&lt;/p>
&lt;p>Data Justice may seem a bit abstract, but it can be applied to all data practices and interactions to engage critically with data. Just remember, data does not exist without us, data is never neutral. Try to have more conversations about data - or just more conversations about anything, because there is nothing that could replace human interactions, not even with all the data in the world.&lt;/p>
&lt;h3 id="reference-list">Reference List&lt;/h3>
&lt;p>Albert, K. and Delano, M. (2021) “This whole thing smacks of gender: Algorithmic exclusion in bioimpedance-based body composition analysis,” in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. New York, NY, USA: ACM.&lt;/p>
&lt;p>Angwin, J., Tobin, A. and Varner, M. (no date) Facebook (still) letting housing advertisers exclude users by race, ProPublica. Available at: &lt;a href="https://www.propublica.org/article/facebook-advertising-discrimination-housing-race-sex-national-origin">https://www.propublica.org/article/facebook-advertising-discrimination-housing-race-sex-national-origin&lt;/a> (Accessed: March 31, 2022).&lt;/p>
&lt;p>Criado Perez, C. (2019) Invisible women: Exposing data bias in a world designed for men. London, England: Chatto &amp;amp; Windus.&lt;/p>
&lt;p>DataGénero (no date) Datagenero.org. Available at: &lt;a href="https://www.datagenero.org/datag%C3%A9nero">https://www.datagenero.org/datag%C3%A9nero&lt;/a> (Accessed: March 30, 2022).&lt;/p>
&lt;p>D’Ignazio, C. and Klein, L. F. (2020) Data Feminism. London, England: MIT Press.&lt;/p>
&lt;p>Dillet, R. (2018) Facebook knows literally everything about you, TechCrunch. Available at: &lt;a href="https://techcrunch.com/2018/03/23/facebook-knows-literally-everything-about-you/?guccounter=1">https://techcrunch.com/2018/03/23/facebook-knows-literally-everything-about-you/?guccounter=1&lt;/a> (Accessed: March 31, 2022).&lt;/p>
&lt;p>Escobar, A. (2018) Designs for the pluriverse: Radical interdependence, autonomy, and the making of worlds. Durham, NC: Duke University Press.&lt;/p>
&lt;p>Gates, M. F. (2020) Sexist and incomplete data hold back the world’s COVID-19 response, Bill &amp;amp; Melinda Gates Foundation. Available at: &lt;a href="https://www.gatesfoundation.org/ideas/articles/stat-melinda-gates-sexist-covid19-data">https://www.gatesfoundation.org/ideas/articles/stat-melinda-gates-sexist-covid19-data&lt;/a> (Accessed: March 30, 2022).&lt;/p>
&lt;p>Gebru, T. et al. (2021) “Datasheets for datasets,” Communications of the ACM, 64(12), pp. 86–92. doi: 10.1145/3458723.&lt;/p>
&lt;p>General Data Protection Regulation (GDPR) – official legal text (2016) General Data Protection Regulation (GDPR). Available at: &lt;a href="https://gdpr-info.eu/">https://gdpr-info.eu/&lt;/a> (Accessed: March 30, 2022).&lt;/p>
&lt;p>Gilbert, J. and Keane, D. (2016) “Equality versus fraternity? Rethinking France and its minorities,” International journal of constitutional law, 14(4), pp. 883–905. doi: 10.1093/icon/mow059.&lt;/p>
&lt;p>Hill, K. (2012) How target figured out A teen girl was pregnant before her father did, Forbes. Available at: &lt;a href="https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/?sh=66a059936668">https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/?sh=66a059936668&lt;/a> (Accessed: March 31, 2022).&lt;/p>
&lt;p>Its Intersex Awareness Day - here are 5 myths we need to shatter (2018) Amnesty International. Available at: &lt;a href="https://www.amnesty.org/en/latest/news/2018/10/its-intersex-awareness-day-here-are-5-myths-we-need-to-shatter/">https://www.amnesty.org/en/latest/news/2018/10/its-intersex-awareness-day-here-are-5-myths-we-need-to-shatter/&lt;/a> (Accessed: March 31, 2022).&lt;/p>
&lt;p>Joris Toonders, Y. (2014) Data is the new oil of the digital economy, WIRED. Available at: &lt;a href="https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/">https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/&lt;/a> (Accessed: March 31, 2022).&lt;/p>
&lt;p>King, C. et al. (2020) “Addressing missing data in substance use research: A review and data justice-based approach: A review and data justice-based approach,” Journal of addiction medicine, 14(6), pp. 454–456. doi: 10.1097/ADM.0000000000000644.&lt;/p>
&lt;p>Lorde, A. (1984) Sister Outsider: Essays and Speeches. Penguin Classics.&lt;/p>
&lt;p>Lotherington, C. (2022).&lt;/p>
&lt;p>McKenzie, K. (2020) RACE AND ETHNICITY DATA COLLECTION DURING COVID-19 IN CANADA: IF YOU ARE NOT COUNTED YOU CANNOT COUNT ON THE PANDEMIC RESPONSE. Available at: &lt;a href="https://rsc-src.ca/en/race-and-ethnicity-data-collection-during-covid-19-in-canada-if-you-are-not-counted-you-cannot-count">https://rsc-src.ca/en/race-and-ethnicity-data-collection-during-covid-19-in-canada-if-you-are-not-counted-you-cannot-count&lt;/a>.&lt;/p>
&lt;p>Real-time personalization and recommendation (no date) Amazon.com. Available at: &lt;a href="https://aws.amazon.com/personalize/">https://aws.amazon.com/personalize/&lt;/a> (Accessed: March 31, 2022).&lt;/p>
&lt;p>Thompson, E. et al. (2021) “COVID-19: A case for the collection of race data in Canada and abroad,” Releve des maladies transmissibles au Canada [Canada communicable disease report], 47(7–8), pp. 300–304. doi: 10.14745/ccdr.v47i78a02.&lt;/p>
&lt;p>Tierra Común (no date) Tierra Común. Available at: &lt;a href="https://www.tierracomun.net/en/home">https://www.tierracomun.net/en/home&lt;/a> (Accessed: March 30, 2022).&lt;/p>
&lt;p>Zuboff, S. (2020) The age of surveillance capitalism the age of surveillance capitalism: The fight for a human future at the new frontier of power. New York, NY: PublicAffairs.&lt;/p></description></item><item><title>>> Digital Deconstruction of Human Rights</title><link>https://lexahl.github.io/maie/digital-deconstruction-of-human-rights/</link><pubDate>Sun, 10 Apr 2022 00:00:00 +0000</pubDate><guid>https://lexahl.github.io/maie/digital-deconstruction-of-human-rights/</guid><description>&lt;p>&lt;a href="https://lexahl.github.io/maie/txt/ddhr.txt" target="_blank">click here for a plain text file of this page →&lt;/a>&lt;/p>
&lt;p>This project critiques human rights as framed by the UN Declaration of Human Rights (UDHR) from 1948. Through a collection of text data about human rights, the imagery and various contexts, this project encourages the viewer to question “who has power to declare rights?” and “who can claim rights?” in a critique of the uneven distribution of rights and the antiquity of certain rights around the world today.&lt;/p>
&lt;p>While the UDHR is and has been used in liberatory movements and to work towards a fair, dignified, equal treatment of individuals and communities around the world, these same rights are a political tool that has also been used to justify imperialism, violence, and more. The UDHR can be seen as a political tool by its framing of the rights and freedoms of individuals in relation to the nation-state. For example, Article 9 states that “No one shall be subjected to arbitrary arrest, detention or exile.” Here, and in many other rights from the UDHR, the nation-state is “paradoxically the perpetrator of human rights violations and the body through which rights and freedoms are given to people” (source: Human Rights and Computation Course UAL CCI MA Internet Equalities, Cindy Ma) This project is a “deconstruction” of this framing by exploring the ideas that technologies, individuals, nature, and corporations can also be granters and violators of human rights.&lt;/p>
&lt;p>&lt;img src="https://lexahl.github.io/img/ddhr1.png" alt="screenshot of ddhr landing page that shows a mix rights from twitter, gpt-2, and UNDHR with images in the background showing an example of that right not being granted" title="DDHR navigation page">
&lt;img src="https://lexahl.github.io/maie/img/ddhr2.png" alt="screenshot of ddhr twitter page with scraped tweets shown" title="DDHR twitter data page">&lt;/p>
&lt;p>‘DDHR’ is a text data project to enhance awareness of the unevenness of human rights in specific contexts, and their entanglements with corporations and technologies through time.|&lt;/p>
&lt;p>&lt;a href="https://ddhr.cargo.site/" target="_blank">EXPLORE THE DDHR DEMO →&lt;/a>&lt;/p>
&lt;h3>project process&lt;/h2>
&lt;p>This project originated in February 2022 with the prompt to create a prototype of a data-driven artefact that explores bias in AI or other computational inequality. The Digital Deconstruction of Human Rights explores the social issues raised by human rights in the digital age using multiple text datasets.&lt;/p>
&lt;p>Over this process, I heard from family and friends about the &amp;ldquo;Freedom Convoy&amp;rdquo; protest in Canada, where people claim human rights violations for vaccine mandates and border-crossing rules. I heard about the Tigray War, the Russia-Ukraine war on Twitter, and I watched these topics, and their entanglements with human rights, take over the headlines and conversations in my circles. It became clearer that some voices are a lot louder than others when speaking about human rights, which led me to think about who can claim human rights, who is more human than who, and who and what can control human rights.&lt;/p>
&lt;p>Twitter text data in this project was &amp;ldquo;scraped&amp;rdquo; selectively. There is hate speech on Twitter, and it did not to be included in this project. Embedding harm such as racism, sexism, hate speech, and other biases is not a new issue with data-based projects. Specifically on Twitter, Microsoft&amp;rsquo;s Tay chatbot, &amp;ldquo;TayTweets,&amp;rdquo; was an example of what could go wrong with using data from Twitter.&lt;/p>
&lt;p>Context: “In 2016, Microsoft’s Racist Chatbot Revealed the Dangers of Online Conversation The bot learned language from people on Twitter—but it also learned values” (source: &lt;a href="https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation">https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation&lt;/a>)&lt;/p>
&lt;p>Originally this project was going to generate “human rights” by fine-tuning GPT-2 to generate context relevant text, based on the data scraped from Twitter. However, as I continued to collected the data from Twitter and started playing around with text generation using GPT-2 and transformers, I noticed that some statements from Twitter and some statements generated with GPT-2 were very similar. Additionally, when reading some of the Twitter-claimed rights, I agreed with sentiment some of them (Everyone is entitled to health care, Source: Twitter, Date: 2/17/2022) and I noticed that many of the rights were supported by others. I understood support on Twitter as demonstrated by likes, retweets, and comments on the post. Another important consideration is that I had no way of knowing if a person authored the tweet or if a bot did. Because of the potenial overlap of computer generated statements, the fact that I could not know if a person had shared the tweet, and the courious similarity, I decided to not continue with fine-tuning GPT-2 to generate text data.&lt;/p>
&lt;p>The text data from Twitter, GPT-2, and UDHR was “cleaned,” meaning that it was modified for conformity and exportability, using Google Sheets. This tool was chosen because of my previous knowledge and experiences with Excel functions, the ease of export, and the way that I can share a viewable link publicly.&lt;/p>
&lt;p>Missing from this project are Tweets in languages besides English. While users from many places around the world can Tweet in English, many voices and opinions are not part of this project. Furthermore, all perspectives and voices that do not share specific views and statements about rights on Twitter are not included. These are some of the loud voices, but these voices often speak over others. As this project is to create a discussion about human rights, I hope it can create space for more perspectives and more voices to be listened to.&lt;/p>
&lt;p>&lt;a href="https://ddhr.cargo.site/" target="_blank">EXPLORE THE DDHR DEMO →&lt;/a>&lt;/p></description></item><item><title>>> Data Biases in Research Workshops</title><link>https://lexahl.github.io/maie/data-biases-in-research-workshops/</link><pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate><guid>https://lexahl.github.io/maie/data-biases-in-research-workshops/</guid><description>&lt;p>&lt;a href="https://lexahl.github.io/maie/txt/databias.txt" target="_blank">click here for a plain text file of this page →&lt;/a>&lt;/p>
&lt;p>Special Guest Lecturer at the London College of Communication in MA Design for Social Innovation and Sustainable Futures Course&lt;/p>
&lt;p>&lt;img src="https://lexahl.github.io/maie/img/teach1.png" alt="photo of me lecturing infront of an engaged audience" title="me teaching">&lt;/p>
&lt;h3>about the workshops&lt;/h2>
&lt;p>Data is everywhere and affects us all - even a conversation with a community member is a way of collecting data. What kind of questions are you asking? What happens with their answers? What are the power dynamics of the conversation? Whom is this project for? Who is this project by? In this workshop, ideas about power, bias, and extractive research methods were discussed and challenged. Different kinds of biases were recognized through various examples and group discussions. Furthermore ways to work with these biases while considering our own power as researchers was explained with community based research as a focus.&lt;/p>
&lt;p>&lt;img src="https://lexahl.github.io/maie/img/workshop1.png" alt="slide 1 of the workshop, green text that says data and research justice over a black and white background with the same text" title="Title">&lt;/p>
&lt;p>&lt;img src="https://lexahl.github.io/maie/img/workshop2.png" alt="a slide from the presentation that has the title questions to recognize some biases. it quotes &amp;amp;ldquo;this is not about answer, but rather about asking better question&amp;amp;rdquo; it asks below, what pre-existing biases do we have about the participants? what are they? if i am doing research with marginalized groups of people, are we centering their trauma without considering their resilience and full humanity? how might the questions that are being asked be seen as discriminating? do they further marginalize particular populations? could the data being collected be used to discriminate based on class, for example? is it necessary to ask about income? why? how are the questions influenced by bias? are we considering other axis of identity? how will the data that is being collected reflect the population the project seeks to serve or speak to? how can we analyze the data from multiple perspectives and intersections of identity?" title="Questions to recognize some biases">&lt;/p></description></item><item><title>>> Migraine Chatbot</title><link>https://lexahl.github.io/maie/migraine-chatbot/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://lexahl.github.io/maie/migraine-chatbot/</guid><description>&lt;p>&lt;a href="https://lexahl.github.io/maie/https://lexahl.github.io/maie/txt/migrainechatbot.txt" target="_blank">click here for a plain text file of this page →&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://lexahl.github.io/maie/img/migraine1.png" alt="screenshot of the chatbot&amp;amp;rsquo;s website homepage" title="my migraines screenshot">&lt;/p>
&lt;p>I designed this chatbot to assist me with talking about my migraines. I’ve had migraines for more than 9 years now. They started when I was 15. I usually get a couple of migraines each week. I know you can’t see my migraines, so I guess that it is hard to understand what they are like and how having migraines affects me.&lt;/p>
&lt;p>&lt;a href="https://lexahl.github.io/migraine/" target="_blank">I DON&amp;rsquo;T NEED TO EXPLAIN THIS ALL TO YOU, BUT I WANT TO →&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://lexahl.github.io/maie/img/migraine2.png" alt="screenshot of the chatbot in the middle of a conversation" title="chatbot conversation screenshot">&lt;/p>
&lt;h3>about this project&lt;/h2>
&lt;p>Feminism, to me, is labour. It is making labour visible, and it is a dialogue. It is having conversations that are uncomfortable, and it is making space for change. This project is about the conversations I have about my migraines and how that in itself is labour. I do not owe it to anyone to explain my migraines, but it is expected of me. From explaining to friends why I can’t see them to requiring documents from 3 countries to prove I have an invisible condition, it is expected of me in society, and it is labour.&lt;/p>
&lt;p>This project is a way to shift labour and shift power. As the chatbot is scripted and the user cannot ask questions that I do not want to answer, the chatbot cannot receive abuse, and I control what is shared in the conversation. However, I do not always have this power. Sometimes my access to support and healthcare depends on how I answer specific questions, and in these cases, I have no power.&lt;/p>
&lt;p>In this project, I am the community that is being represented. I am the expert, and I am the designer. My values are being shared, and my experiences are being prioritized. I am benefiting from the design, and this project was fully co-created to address the problem I am hoping to solve. I do not want to have these conversations anymore, and I do not have to. Feminism, to me, prioritizes is inclusion and diversity. It means making a website that is accessible, and it means making a website that can represent me. As Fritsch and Hamraie state in the Crip Technoscience Manifesto (2019), “disabled people are experts and designers of everyday life.” This is my everyday life, and these are my migraines.&lt;/p>
&lt;p>&lt;a href="https://lexahl.github.io/migraine/" target="_blank">I DON&amp;rsquo;T NEED TO EXPLAIN THIS ALL TO YOU, BUT I WANT TO →&lt;/a>&lt;/p>
&lt;center>
&lt;div>
&lt;iframe
src="https://instagram.com/p/CaXASb_MX2k/embed"
frameborder="0"
allowfullscreen
scrolling="no"
allowtransparency
width="320"
height="520"
>&lt;/iframe>
&lt;/div>&lt;/center></description></item><item><title>>> Intersectional Internets Zine Work</title><link>https://lexahl.github.io/maie/intersectional-internets-zine-work/</link><pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate><guid>https://lexahl.github.io/maie/intersectional-internets-zine-work/</guid><description>&lt;p>&lt;a href="https://lexahl.github.io/maie/txt/intersectionalinternets.txt" target="_blank">click here for a plain text file of this page →&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://lexahl.github.io/maie/img/zine1.png" alt="black and white zine artwork" title="welcome to the next chapter of social connection">&lt;/p>
&lt;h3>about this project&lt;/h3>
&lt;p>Using hand-written text and visuals from meta’s products, this work hypnotically responds to the sociotechnical imaginary of the future proposed by meta’s metaverse. This work also reflects on some of the themes relating to workers&amp;rsquo; (lack of) control over the (technological) means of production, in discussing this imagined labour future. &lt;/p>
&lt;p>This piece engages with the questions: Who is included/excluded from this imaginary? Whose imaginary is taken as an authority? Whose interests are being served? &lt;/p>
&lt;p>The process of creating this work involved analyzing the text and asking questions, thinking about Ruha Benjamin’s book Race After Technology, Katherine Ye&amp;rsquo;s essay Silicon Valley and the English Language, Sheila Jasanoff and Sang-Hyun Kim’s book Dreamscapes of Modernity, and more. This project started with tweets to support the analysis of the sociotechincal imaginary of meta’s (facebook’s) metaverse, but then I realized that the tweets and the language being used in them are also imaginaries.&lt;/p>
&lt;p>This work will be published online (soon!) in a collaborative zine with MA students from the Creative Computing Institute and PhD students at the Oxford Internet Institute.&lt;/p>
&lt;center>
&lt;div>
&lt;iframe
src="https://instagram.com/p/CXHDI2YrVml/embed"
frameborder="0"
allowfullscreen
scrolling="no"
allowtransparency
width="320"
height="520"
>&lt;/iframe>
&lt;/div>&lt;/center>
&lt;h3>v2 (Apr 2022)&lt;/h3>
&lt;p>&lt;img src="https://lexahl.github.io/maie/img/zine2.jpg" alt="black and white zine artwork" title="who belongs on the moon? // who belongs in the future?">&lt;/p></description></item></channel></rss>